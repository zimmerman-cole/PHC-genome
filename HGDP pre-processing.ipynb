{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output, display\n",
    "from ipywidgets import IntProgress, IntText, Text\n",
    "\n",
    "data_path = 'HGDP/hgdp/HGDP_FinalReport_Forward.txt'\n",
    "pops_path = 'HGDP/hgdp/HGDP-CEPH-ID_populations.csv'\n",
    "n, m = 1043, 660918\n",
    "\n",
    "pops = pd.read_csv(pops_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Read and pre-process data SNP by SNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27abfb4fc0ab4acd8877a221900efcba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntText(value=1000, description='dump_size:')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dsizebox = IntText(value=1000, description='dump_size:')\n",
    "display(dsizebox)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "chunk_size = 1\n",
    "dump_size = dsizebox.value\n",
    "\n",
    "data = np.zeros((n, 0))\n",
    "log = []\n",
    "\n",
    "part = 0\n",
    "log_num = 0\n",
    "snp_names = []\n",
    "\n",
    "progress = IntProgress(value=0, max=m, description='0.00%')\n",
    "num_used = IntText(value=0)\n",
    "text_box = Text(value='dump_size=%d' % dump_size)\n",
    "\n",
    "display(progress)\n",
    "display(num_used)\n",
    "display(text_box)\n",
    "\n",
    "last_processed = (-1, None)\n",
    "try:\n",
    "    for i, chunk in enumerate(\n",
    "        pd.read_csv(data_path, sep='\\t', index_col=0, chunksize=chunk_size)\n",
    "    ):\n",
    "\n",
    "        # ====================================================================\n",
    "        # FIRST, verify whether we want to use this SNP in our experiments\n",
    "        bases = defaultdict(int)\n",
    "\n",
    "        skip = False\n",
    "        skip_reason = None\n",
    "        # (s_list is length-one; todo: change to chunk.values[0] w/ no loop)\n",
    "        for s_list in chunk.values:\n",
    "            for s in s_list:\n",
    "                assert len(s) == 2, 's = %s' % s\n",
    "\n",
    "                # don't include SNPs with missing data\n",
    "                if '-' in s:\n",
    "                    skip = True\n",
    "                    skip_reason = \"contains '-'\"\n",
    "                    break\n",
    "                else:\n",
    "                    bases[s[0]] += 1\n",
    "                    bases[s[1]] += 1\n",
    "\n",
    "                # only include SNPs that have exactly \n",
    "                # 2 observed base pairs across the population\n",
    "                if len(bases.keys()) > 2:\n",
    "                    skip = True\n",
    "                    skip_reason = \"contains 3+ bases\"\n",
    "                    break\n",
    "\n",
    "            if skip:\n",
    "                break\n",
    "\n",
    "        # don't use alleles which only take one base pair\n",
    "        if len(bases.keys()) == 1 or skip:\n",
    "            if skip_reason is None:\n",
    "                skip_reason = 'num bases = 1'\n",
    "\n",
    "            progress.value += 1\n",
    "            progress.description = '%.2f%%' % 100 * (progress.value / m)\n",
    "            log.append('Skipping %s - %s' % (chunk.index[0], skip_reason))\n",
    "            last_processed = (i, chunk.index[0])\n",
    "            continue\n",
    "\n",
    "        snp_names.append(chunk.index[0])\n",
    "        # ====================================================================\n",
    "        # SECOND, encode the SNP values in {0, 1, 2} and add to design matrix\n",
    "        bk, bv = list(bases.keys()), list(bases.values())\n",
    "        # use major allele as reference allele\n",
    "        maj_all = bk[0] if bv[0] >= bv[1] else bk[1]\n",
    "#         print(maj_all)\n",
    "\n",
    "        # use 8-bit integer to save memory (since we only need to encode {0, 1, 2})\n",
    "        new_col = np.zeros((n, 1), dtype=np.int8) \n",
    "\n",
    "        for r_num, s in enumerate(chunk.values[0]):\n",
    "            new_col[r_num, 0] = len([b for b in s if b == maj_all])\n",
    "\n",
    "        data = np.hstack([data, new_col])\n",
    "\n",
    "        num_used.value += 1\n",
    "        progress.value += 1\n",
    "        progress.description = '%.2f%%' % (progress.value / m)\n",
    "        log.append('Keeping %s' % chunk.index[0])\n",
    "        last_processed = (i, chunk.index[0])\n",
    "\n",
    "        # periodically write log to disk, clear from memory\n",
    "        if len(log) == 10000:\n",
    "            log_path = 'data/logs/logpart=%d_dsize=%d.txt' % (log_num, dump_size)\n",
    "\n",
    "            f = open(log_path, 'w')\n",
    "            for line in log:\n",
    "                f.write(line+'\\n')\n",
    "            f.close()\n",
    "\n",
    "            log = []\n",
    "\n",
    "            text_box.value = 'Log part %d written (i=%d; ds=%d)...' \\\n",
    "                                % (log_num, i, dump_size)\n",
    "            log_num += 1\n",
    "\n",
    "        # periodically write processed data to disk, clear from memory\n",
    "        if data.shape[1] == dump_size: \n",
    "\n",
    "            to_save = pd.DataFrame(\n",
    "                data=data, index=chunk.columns, columns=snp_names\n",
    "            )\n",
    "            snp_names = []\n",
    "\n",
    "            filepath = 'data/part=%d_dsize=%d.csv' % (part, dump_size)\n",
    "\n",
    "            to_save.to_csv(filepath)\n",
    "            data = np.zeros((n, 0))\n",
    "\n",
    "            text_box.value = 'Part %d of the data is saved (i=%d; ds=%d).' \\\n",
    "                                    % (part, i, dump_size)\n",
    "            part += 1\n",
    "\n",
    "\n",
    "    if data.shape[1] > 0:\n",
    "        print('Saving part %d of the data (last part).' % part)\n",
    "        to_save = pd.DataFrame(\n",
    "            data=data, index=chunk.columns, columns=snp_names\n",
    "        )\n",
    "        snp_names = []\n",
    "\n",
    "        filepath = 'data/part=%d_dsize=%d.csv' % (part, dump_size)\n",
    "\n",
    "        to_save.to_csv(filepath)\n",
    "\n",
    "    if len(log) > 0:\n",
    "        log_path = 'data/logs/logpart=%d_dsize=%d.txt' % (part, dump_size)\n",
    "        f = open(log_path, 'w')\n",
    "        for line in log:\n",
    "            f.write(line+'\\n')\n",
    "        f.close()\n",
    "except KeyboardInterrupt:\n",
    "    if len(log) > 0:\n",
    "        log_path = 'data/logs/logpart=%d_dsize=%d.txt' % (part, dump_size)\n",
    "        f = open(log_path, 'w')\n",
    "        for line in log:\n",
    "            f.write(line+'\\n')\n",
    "        f.close()\n",
    "    \n",
    "    f = open('data/logs/LASTPROCESSED_dsize=%d.txt' % dump_size, 'w')\n",
    "    f.write('i=%d; name=%s' % (last_processed[0], last_processed[1]))\n",
    "    f.close()\n",
    "    \n",
    "progress.close()\n",
    "num_used.close()\n",
    "text_box.close()\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# not sure how to concatenate CSVs column-wise without loading \n",
    "# them into memory, so here goes...\n",
    "\n",
    "load = lambda fn: pd.read_csv(fn).rename(\n",
    "    {'Unnamed: 0': 'CEPH ID'}, axis='columns'\n",
    ").set_index('CEPH ID').astype(np.int8)\n",
    "\n",
    "full_df = load('data/part=0_dsize=1000.csv')\n",
    "\n",
    "for part_num in tqdm(range(1, 500)):\n",
    "    try:\n",
    "        full_df = pd.concat(\n",
    "            [full_df, load('data/part=%d_dsize=1000.csv' % part_num)], axis=1\n",
    "        )\n",
    "    except FileNotFoundError:\n",
    "        break\n",
    "        \n",
    "full_df.to_csv('full_cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine w/ population/geographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/.virtualenvs/RL/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mStopIteration\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6c369910609c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcolumn_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'full_cleaned_data.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# for i, chunk in enumerate(reader):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     print(chunk.columns)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/RL/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/RL/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/RL/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1034\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'skipfooter not supported for iteration'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1036\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/RL/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1862\u001b[0m                 col_dict = dict(filter(lambda item: item[0] in columns,\n\u001b[0;32m-> 1863\u001b[0;31m                                        col_dict.items()))\n\u001b[0m\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1865\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/RL/lib/python3.5/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(item)\u001b[0m\n\u001b[1;32m   1860\u001b[0m                     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filter_usecols\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1862\u001b[0;31m                 col_dict = dict(filter(lambda item: item[0] in columns,\n\u001b[0m\u001b[1;32m   1863\u001b[0m                                        col_dict.items()))\n\u001b[1;32m   1864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "column_names = pd.read_csv('full_cleaned_data.csv', nrows=0)\n",
    "print(column_names)\n",
    "\n",
    "# for i, chunk in enumerate(reader):\n",
    "#     print(chunk.columns)\n",
    "#     input()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
